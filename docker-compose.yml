services:
  postgres:
    image: postgres:16
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 10s
      retries: 10
      start_period: 10s
    restart: unless-stopped

  redis:
    image: redis:7
    restart: unless-stopped

  airflow-init:
    build: .
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_started

    environment: &airflow-common-env
      AIRFLOW_UID: ${AIRFLOW_UID:-1001}
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow

      PYTHONPATH: /opt/airflow/src

      # Auth (Airflow 3 - SimpleAuthManager)
      AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_USERS: "admin:admin,paulo:admin"
      AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_PASSWORDS_FILE: /opt/airflow/auth/passwords.json

      # Config file path (fica em volume nomeado, permissões OK)
      AIRFLOW_CONFIG: /opt/airflow/config/airflow.cfg

      # Logs
      AIRFLOW__LOGGING__BASE_LOG_FOLDER: /opt/airflow/logs
      AIRFLOW__LOGGING__LOGGING_LEVEL: INFO

      # QoL
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__CORE__DEFAULT_TIMEZONE: America/Sao_Paulo

      AIRFLOW__CORE__EXECUTION_API_SERVER_URL: "http://airflow-api-server:8080/execution/"
      AIRFLOW__WEBSERVER__BASE_URL: "http://localhost:8080"

    volumes: &airflow-common-vols
          - ./dags:/opt/airflow/dags
          - ./logs:/opt/airflow/logs
          - ./plugins:/opt/airflow/plugins
          - ./config:/opt/airflow/config # Mudei para bind mount também
          - ./auth:/opt/airflow/auth     # Mudei de volume nomeado para pasta local
          - ./src:/opt/airflow/src

    # Init SEM db check (db check é o que estava disparando o create do cfg)
    command: >
      bash -c "
        mkdir -p /opt/airflow/logs /opt/airflow/dags /opt/airflow/plugins /opt/airflow/config /opt/airflow/auth &&
        airflow db migrate
      "
    restart: "no"

  airflow-api-server:
    build: .
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    environment: *airflow-common-env
    volumes: *airflow-common-vols
    ports:
      - "8080:8080"
    command: airflow api-server
    restart: unless-stopped

  airflow-scheduler:
    build: .
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    environment: *airflow-common-env
    volumes: *airflow-common-vols
    command: airflow scheduler
    restart: unless-stopped

  airflow-dag-processor:
    build: .
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    environment: *airflow-common-env
    volumes: *airflow-common-vols
    command: airflow dag-processor
    restart: unless-stopped

  airflow-triggerer:
    build: .
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    environment: *airflow-common-env
    volumes: *airflow-common-vols
    command: airflow triggerer
    restart: unless-stopped

  airflow-worker:
    build: .
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    environment: *airflow-common-env
    volumes: *airflow-common-vols
    command: >
      bash -lc '
        exec airflow celery worker
        --loglevel=INFO
        --hostname=airflow-worker@%h
      '
    restart: unless-stopped

  flower:
    build: .
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    environment: *airflow-common-env
    volumes: *airflow-common-vols
    ports:
      - "5555:5555"
    command: >
      bash -lc '
        exec airflow celery flower
        --port=5555
        --loglevel=INFO
      '
    restart: unless-stopped

volumes:
  postgres-db-volume: